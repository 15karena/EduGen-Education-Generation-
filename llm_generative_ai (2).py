# -*- coding: utf-8 -*-
"""LLM_Generative_AI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Maxiy3BnJo-KaskD4HCbPR97v9vykUU
"""

!pip install anvil-uplink

import anvil.server

anvil.server.connect("server_PY2V7TZFD66TOZTLEGJAMNOS-NORKY4Y3OWKPF3RT")

#!pip show grpcio

#!pip uninstall grpcio

#!pip install grpcio==1.60.0

!pip install pymilvus langchain langchain-community milvus transformers accelerate einops sentence-transformers pypdf bitsandbytes -U langchain-huggingface

from langchain_community.document_loaders import PyPDFLoader
from langchain_community.document_loaders import CSVLoader
from langchain_community.document_loaders import PyPDFDirectoryLoader
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores.milvus import Milvus
from langchain_community.embeddings import HuggingFaceBgeEmbeddings
from langchain.prompts import PromptTemplate
from langchain_core.prompts import ChatPromptTemplate
from langchain.chains import RetrievalQA
from langchain import HuggingFaceHub
from langchain_community.llms import Ollama
from langchain.llms import CTransformers
import os
import torch
import transformers
from langchain_community.vectorstores.milvus import Milvus
from langchain_core.runnables import RunnablePassthrough
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser
from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline
from langchain.document_loaders.csv_loader import CSVLoader
from langchain.embeddings import HuggingFaceEmbeddings
from langchain.chains import QAGenerationChain
from langchain_core.pydantic_v1 import BaseModel, Field

from milvus import default_server
default_server.start()

# STEP 1. CONNECT TO MILVUS STANDALONE DOCKER.

import pymilvus, time
from pymilvus import (
    MilvusClient, utility, connections,
    FieldSchema, CollectionSchema, DataType, IndexType,
    Collection, AnnSearchRequest, RRFRanker, WeightedRanker
)
print(f"Pymilvus: {pymilvus.__version__}")

# Connect to the local server.
connection = connections.connect(
  alias="default",
  host='localhost', # or '0.0.0.0' or 'localhost'
  port='19530'
)

# Get server version.
print(utility.get_server_version())

# Check the collection using MilvusClient.
mc = MilvusClient(connections=connection)

from huggingface_hub import notebook_login
notebook_login()

from google.colab import userdata
huggingfacehub_api_key = userdata.get('HUGGINGFACE_API_KEY')

from langchain.embeddings import HuggingFaceEmbeddings
embedding_model_name = "sentence-transformers/all-MiniLM-L6-v2"
embeddings = HuggingFaceEmbeddings(
    model_name = embedding_model_name
)

from pymilvus import MilvusClient, DataType

# 1. Set up a Milvus client
client = MilvusClient(
    uri="http://localhost:19530"
)

# 2. Create a collection in quick setup mode
client.create_collection(
    collection_name="rag_llm_2",
    dimension=1536
)

res = client.get_load_state(
    collection_name="rag_llm_2"
)

print(res)

# Membuat direktori
import os
os.mkdir("/content/Data")

loader = CSVLoader(file_path="/content/Data/course_184_modules_2.csv", encoding="utf-8-sig", csv_args={'delimiter':','})
documents = loader.load()
print(documents)

text_splitter = RecursiveCharacterTextSplitter(
    chunk_size=1000,
    chunk_overlap=200,
    length_function=len,
    is_separator_regex = False
)

text_chunk = text_splitter.split_documents(documents)

len(text_chunk)

vectorstore = Milvus.from_documents(
    documents=text_chunk,
    embedding=embeddings,
    connection_args={
        "uri": "http://localhost:19530",
    },
    drop_old=True,
)

vectorstore

retriever = vectorstore.as_retriever()

query = "apa itu python?"
result = vectorstore.similarity_search(query, k=2)
result

from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline


# model_id = "microsoft/phi-1_5"
# model_config = transformers.AutoConfig.from_pretrained(
#     model_id,
# )

# model = transformers.AutoModelForCausalLM.from_pretrained(
#     model_id,
#     trust_remote_code=True,
#     config=model_config,
#     device_map='auto',
# )

# tokenizer = AutoTokenizer.from_pretrained(model_id)

# model_id = "microsoft/phi-1_5"
# #microsoft/phi-1_5
# #meta-llama/Llama-2-7b-hf
# #SweatyCrayfish/llama-3-8b-quantized
# model = AutoModelForCausalLM.from_pretrained(model_id)
# tokenizer = AutoTokenizer.from_pretrained(model_id)

# # model = HuggingFaceHub(
# #     huggingfacehub_api_token=huggingfacehub_api_key,
# #     repo_id="MaziyarPanahi/Mistral-7B-Instruct-v0.3-GGUF",
# #     #repo_id="cahya/gpt2-small-indonesian-522M",
# #     model_kwargs={"temperature":0.5}
# # )


model_id = "meta-llama/Meta-Llama-3-8B-Instruct"
bnb_config = transformers.BitsAndBytesConfig(
    load_in_4bit=True,
    bnb_4bit_quant_type='nf4',
    bnb_4bit_use_double_quant=True,
    bnb_4bit_compute_dtype=torch.bfloat16,
)


model_config = transformers.AutoConfig.from_pretrained(
   model_id,
)

model = transformers.AutoModelForCausalLM.from_pretrained(
    model_id,
    trust_remote_code=True,
    config=model_config,
    device_map='auto',
    quantization_config=bnb_config
)

tokenizer = AutoTokenizer.from_pretrained(model_id)

eos_token_id = tokenizer.eos_token_id

generate_text = transformers.pipeline(
    model=model,
    tokenizer=tokenizer,
    task="text-generation",
    return_full_text=True,
    temperature=1.0,
    max_length=1024,
    do_sample=True,
    pad_token_id=eos_token_id

)

from langchain_huggingface.llms import HuggingFacePipeline
llm = HuggingFacePipeline(pipeline=generate_text)

"""# MVP 1"""

def generate_module(user_input):

    template = f"""
    You are an IT teacher and you have to create a learning module based on the following user input: {user_input}.
    Explains the learning module completely and is easy to understand.
    If you don't know the answer, just say that you don't know, don't try to make up an answer and don't produce repetitive answers.
    Make sure your answer in Indonesian and create learning modules into paragraphs.
    Make sure you just have one answer

    The example of your the answer :
    Python adalah bahasa pemrograman yang sangat populer dan sering digunakan dalam berbagai bidang. Diciptakan pada tahun 1991 oleh Guido van Rossum, Python menonjol karena sintaks yang sederhana dan mudah dipahami, serta penekanan pada keterbacaan kode. Bahasa ini mendukung berbagai paradigma pemrograman, termasuk pemrograman berorientasi objek, fungsional, dan terstruktur. Python juga dikenal sebagai bahasa yang dinamis, yang berarti Anda tidak perlu mendeklarasikan tipe variabel secara eksplisit. Selain itu, Python memiliki ekosistem yang kaya, dengan berbagai pustaka standar dan alat yang tersedia untuk pengembangan perangkat lunak, ilmu data, kecerdasan buatan, pengembangan web, dan banyak lagi. Karena sifatnya yang open source, Python menjadi pilihan yang populer bagi pengembang di seluruh dunia, baik mereka yang baru belajar pemrograman maupun yang berpengalaman.

    Answer :
    """

    # Generate the multiple-choice answers
    result = generate_text(template, max_length=1024, num_return_sequences=1)
    generated_text = result[0]['generated_text']

    answer_start = generated_text.find("Answer :")
    answers = generated_text[answer_start:].strip()

    answers = answers.replace("\n\n", " ").strip()

    return answers

user_input = str(input("user_input: "))
result = generate_module(user_input)

result

"""# MVP 2"""

def generate_quiz(quiz_context):
  num_questions = 10
  prompt_assessment  = """
  You are an expert quizzes maker for technical fields. Let's think step by step and
  create a quizzes with {num_questions} and multiple choice questions about the following concept/content: {quiz_context}.
  Answer according to the quiz format and make sure your answer is in Indonesian.
  Do not make repetitive questions and just make multiple choice questions.
  Stop making questions when you have made {num_questions} questions


  The format of the quiz should be as follows:
  - Questions:
      1. <Question1>
          a. <Answer 1>
          b. <Answer 2>
          c. <Answer 3>
          d. <Answer 4>
          Answer: <a|b|c|d>
      2. <Question2>
          a. <Answer 1>
          b. <Answer 2>
          c. <Answer 3>
          d. <Answer 4>
          Answer: <a|b|c|d>
      ...

  Example:
  - Questions:
      1. Berapa kompleksitas waktu dari binary search tree?
          a. O(n)
          b. O(log n)
          c. O(n^2)
          d. O(1)
          Answer: b

Here the questions :

  """
  promptt = ChatPromptTemplate.from_template(prompt_assessment)
  final_prompt = promptt.format(
      quiz_context = quiz_context,
      num_questions = num_questions

  )
  # Generate the multiple-choice answers
  result = generate_text(final_prompt, max_length=1024, num_return_sequences=1)
  generated_text = result[0]['generated_text']

  # Extract the generated answers
  answer_start = generated_text.find("Here the questions :")
  answers = generated_text[answer_start:].strip()

  return answers

quiz_context = str(input("Context : "))
answer = generate_quiz(quiz_context)
answer

"""# MVP 3"""

def generate_chatbot(user_input):
  PROMPT_TEMPLATE =  """Use the following pieces of context to answer the question at the end.
  If you don't know the answer, just say that you don't know, don't try to make up an answer and don't produce repetitive answers.
  Make sure not to create any other questions aside from {user_input}
  Make sure your answer in Indonesian

  {context}

  Question: {user_input}
  Answer :
  """

  prompt = PromptTemplate(
      template=PROMPT_TEMPLATE, input_variables=["context", "user_input"]
  )

  def format_docs(documents):
    return "\n\n".join(doc.page_content for doc in documents)

  output_parser = StrOutputParser()

  rag_chain = (
    {"context": retriever | format_docs,"user_input": RunnablePassthrough()}
    | prompt
    | llm
    | StrOutputParser()
  )

  answer = rag_chain.invoke(user_input)

  #final_answer = answer.find("Answer :")
  answer_start = answer.find("Answer :")
  final_answer = answer[answer_start:].strip()

  return final_answer

answer = generate_chatbot(user_input)
answer

def main():
    user_input = str(input("user_input: "))  # Menerima input dari pengguna dan mengubahnya menjadi huruf kecil

    if "materi" in user_input:
        # Jika input mengandung kata "materi", panggil fungsi generate_module
        user_input = user_input.replace("buatlah materi tentang", "").strip()
        result = generate_module(user_input)
        print(result)
    elif "soal" in user_input:
        # Jika input mengandung kata "soal", panggil fungsi generate_quiz
        user_input = user_input.replace("buatlah soal", "").strip()
        result = generate_quiz(user_input)
        print("Hasil Generate Quiz:", result)
    elif "apa" in user_input:
        result = generate_chatbot(user_input)
        print(result)


if __name__ == "__main__":
    main()

#Anvil caller
@anvil.server.callable
def chat_answer(user_input):
    if "materi" in user_input:
        # Jika input mengandung kata "materi", panggil fungsi generate_module
        user_input = user_input.replace("buatlah materi tentang", "").strip()
        answer = generate_module(user_input)
        return answer
    elif "soal" in user_input:
        # Jika input mengandung kata "soal", panggil fungsi generate_quiz
        user_input = user_input.replace("buatlah soal", "").strip()
        answer = generate_quiz(user_input)
        return answer
    elif "apa" in user_input:
        answer = generate_chatbot(user_input)
        return answer

#anvil server waiter
anvil.server.wait_forever()

# Menyimpan tokenizer dan model ke dalam format .bin
model.save_pretrained('model')
tokenizer.save_pretrained('model')

# Mengunduh file model ke komputer lokal
from google.colab import files

#files.download('model/pytorch_model.bin')
files.download('model/config.json')
files.download('model/tokenizer_config.json')
files.download('model/tokenizer.json')
files.download('model/model.safetensors.index.json')
files.download('model/generation_config.json')
files.download('model/special_tokens_map.json')